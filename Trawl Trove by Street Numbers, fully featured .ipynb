{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This cell installs all the modules needed for this code. You only need to run it once. When running the code again, start it as cell two below.\n",
    "\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install wordcloud\n",
    "!{sys.executable} -m pip install TextBlob\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n",
    "!{sys.executable} -m pip install vega\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716437e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#This code allows you to automatically search historical newspapers via successive street addresses using Trove's API. \n",
    "#You just enter in the street you are searching for, the street numbers you wish to look between, the year span you wish to explore, and your Trove API key and the code does the rest.\n",
    "\n",
    "#The code finds newspapers article via your specifications, automatically culls irrelevant results, and creates property and street level CSV files containing article information and full text. The CSV files  are saved to a newly created folder on your computer named after your search terms, i.e. Little_Lonsdale_street_1870_1890.\n",
    "\n",
    "#In addition, the code graphs article frequency by date and street number, displays an on-screen summary of all the articles found on a street level. It produces street level Wordclouds of the most common words in the article text and in the heading text and displays the 20 most common Ngrams on a street level. All of this material is saved to your directory.\n",
    "#The code also attempts to extract all the people mentioned in the articles and adds them to your CSV files. Please note this work's via Stanford Universityâ€™s Spacy AI that was trained on modern American webcontent. It is not perfect with nineteenth-century material and will often overlook non-American sounding names. As always, machine learning based code reflects the biases of the material it was trained on.\n",
    "\n",
    "#have fun,\n",
    "\n",
    "#Pete \n",
    "\n",
    "\n",
    "\n",
    "#To successfully run the code, you need an individual TROVE API key. You can acquire a key by signing up as a registered user.\n",
    "#Sign up for Trove here: https://trove.nla.gov.au/\n",
    "#Once you have signed up:\n",
    "                        log in to Trove, select your username and select My Profile;\n",
    "                        select the:  For developers tab;\n",
    "                        fill in the form to apply for a Trove API key;\n",
    "                        read the documentation and start using your key to access the API;\n",
    "                        insert your API key into the code below as instructed.\n",
    "\n",
    "\n",
    "# Insert your individual TROVE API key between the quotation marks below.\n",
    "api_key = ''\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import vega\n",
    "import altair as alt\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Enter your search terms below \n",
    "\n",
    "\n",
    "# define street search term\n",
    "\n",
    "street = ['Elizabeth  Street']\n",
    "\n",
    "\n",
    "\n",
    "#state to seatch within\n",
    "#accepted options are: 'ACT', 'International', 'National', '\n",
    "                    #New South Wales', 'Northern Territory', 'Queensland', 'South Australia', 'Tasmania', 'Victoria'\n",
    "\n",
    "state = ['Victoria']\n",
    "\n",
    "#street numbers to be searched between\n",
    "\n",
    "\n",
    "First_number = 1\n",
    "\n",
    "Last_number = 10\n",
    "\n",
    "#dates to search between - year - month -date \n",
    "\n",
    "start_date = 1914\n",
    "end_date = 1919\n",
    "\n",
    "#type of articles to search for, the accepted options are:\n",
    "                                            #'Article';'Advertising';'Details lists, results, guides';'Family Notices'; 'Literature'\n",
    "    \n",
    "#Please note that the API can only search for one article type at a time, or for all article types. \n",
    "#Leave the square brackets below empty if you want to search for all article types\n",
    "\n",
    "articlegenre = ['Article']\n",
    "\n",
    "# rs = the Trove Relevance Score below which this code will cull the results as irrelevant.\n",
    "#If you are finding few results try lowering th RTscore, if you getting false positives try raising the score. \n",
    "#My experiments have shown me that 5 is a  good default rs score. \n",
    "\n",
    "rs = 5\n",
    "\n",
    "# search parameters\n",
    "#generally do not mess with this stuff\n",
    "\n",
    "params = {\n",
    "    'key': api_key,\n",
    "    'zone': 'newspaper',\n",
    "    'include': 'articleText',\n",
    "    'n': 100,\n",
    "    'encoding': 'json',\n",
    "    'bulkHarvest':'false',\n",
    "    'reclevel': 'brief',\n",
    "    'sortby': 'relevance',\n",
    "    'l-state':state,\n",
    "    'l-category':articlegenre \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for street in street:\n",
    "\n",
    "# Slugify street names to use in paths and filenames\n",
    "    street_slug = street.replace(' ', '_')\n",
    "\n",
    "# pathlib makes working with files and directories easier\n",
    "    street_path = Path((street_slug)+\"_\" +str(start_date)+\"_\" + str(end_date))\n",
    "    street_path.mkdir(exist_ok=True)\n",
    "    street_name_dates = (street_slug)+\"_\" +str(start_date)+\"_\" + str(end_date)\n",
    "    \n",
    "\n",
    "for num in range((First_number), (Last_number)):\n",
    "\n",
    "    # Use text: and ~0 to make the search as exact as possible\n",
    "    params['q'] = f'text:\"{num} {street}\"~0  date:[{start_date} TO {end_date}]'\n",
    "    string_num = str(num)\n",
    "# Get the data from the API\n",
    "    response = requests.get(\n",
    "        'https://api.trove.nla.gov.au/v2/result', params=params)\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    time.sleep(.2)\n",
    "\n",
    "    try:\n",
    "        articles = data['response']['zone'][0]['records']['article']\n",
    "    except KeyError:\n",
    "        continue\n",
    "    else:\n",
    "        df = pd.json_normalize(articles)\n",
    "        pd.set_option('max_colwidth', 100000) #important to allow proper searching\n",
    "        \n",
    "# culling returns based on relevence score\n",
    "\n",
    "        df[\"relevance\"] = df[\"relevance.score\"].astype('float')\n",
    "\n",
    "        df = df[df.relevance >= (rs)]\n",
    "#inserting search term column\n",
    "        df.insert(loc=0, column='search_term', value=(\n",
    "            (string_num) + (\" \") + (street_slug)))\n",
    "\n",
    "# ending loop if row empty and results null\n",
    "\n",
    "        shape = df.shape\n",
    "        shape = shape[0]\n",
    "\n",
    "    if (shape) >= (1):\n",
    "\n",
    "        try:\n",
    "# droppinguseless columns\n",
    "            df.drop(columns=['url', 'pageSequence', 'title.id',\n",
    "                    'relevance.score', 'relevance.value', 'snippet'], inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "# converting dftypes\n",
    "            df['id'] = df['id'].astype('int')\n",
    "\n",
    "            df['date'] = pd.to_datetime((df['date']), yearfirst=False,)\n",
    "            \n",
    "       \n",
    "        try:\n",
    "# stripping out HTML\n",
    "            df['article_text'] = df['articleText'].str.replace(\n",
    "                r'<[^<>]*>', '', regex=True)\n",
    "            df.drop(columns='articleText', inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "       \n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "\n",
    "    \n",
    "# relabelling and reordering columns\n",
    "            df['article_ID'] = df['id']\n",
    "            df['article_type'] = df['category']\n",
    "            df['article_heading'] = df['heading']\n",
    "            df['newspaper'] = df['title.value']\n",
    "            df.drop(columns=['id', 'heading', 'title.value'], inplace=True)\n",
    "            df.insert(loc=10, column='people_in_text', value =(\" \"))\n",
    "            \n",
    "            df = df[[\"search_term\",'article_ID', \"relevance\", \"article_type\", \"article_heading\",\n",
    "                     \"newspaper\", \"date\", \"page\", \"troveUrl\", \"article_text\", \"people_in_text\"]]\n",
    "            \n",
    "#finding names of people, inserting into DF\n",
    "            \n",
    "\n",
    "        for number in range(len(df)):\n",
    " \n",
    "                article_text_value =df.loc[[(number)],['article_text']]\n",
    "                article_text_value =str(article_text_value)\n",
    "                \n",
    "                \n",
    "                doc = nlp(article_text_value)\n",
    "                    \n",
    "                people = ([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'PERSON'])\n",
    "                people  = re.sub(r'<[^<>]*>','',str(people))\n",
    "                df.loc[[(number)],['people_in_text']] = str(people)\n",
    "     \n",
    "     \n",
    "  # savingfile based on actual results being found, does not save null results\n",
    "\n",
    "        df.to_csv(Path(street_path, f'{num}_{street_slug}.csv'), index=False)\n",
    "        print(\"Relevant articles were found for\" + \" \" + (string_num) + (\" \") + (street_slug))\n",
    "\n",
    "print(\"Relevant articles were found the street addresses above  and saved as individual csv files  in a\" + \" \" + street_name_dates + \" \" + \"folder in your active path.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening created files, making consolidated street level file\n",
    "\n",
    "\n",
    "list_of_dataframes = []\n",
    "\n",
    "for num1 in range((First_number), (Last_number)):\n",
    "\n",
    "    try:\n",
    "        list_of_dataframes.append(pd.read_csv(\n",
    "            Path(street_path, f'{num1}_{street_slug}.csv')))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "\n",
    "        merged_df = pd.concat(list_of_dataframes)\n",
    "        merged_df.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "merged_df.to_csv(Path(street_path, f'{street_name_dates}.csv'), index=False)\n",
    "\n",
    "print(\"A consolidated CSV file of all the results for\" + \" \" + street_name_dates + \" \" + \" has been created and saved in a\" + \" \"+(street_name_dates) +\" \"+ \"folder in active path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fef88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating graph of article frequency by date, useful for seeing when news was being produced on a street\n",
    "alt.renderers.enable('default')\n",
    "\n",
    "value_counts = merged_df['date'].value_counts()\n",
    "\n",
    "print (('Article numbers by date for')+\" \" + (street) + ' ' + 'from' +\" \"+ str(start_date)+\" \" + 'to' + \" \" + str(end_date))\n",
    "\n",
    "print( \" \")\n",
    "\n",
    "print ('Hover over the data point with mouse to get precise details, scroll sideways to see full dates.')\n",
    "       \n",
    "print ('This graph can be saved using the three dot tool in the upper right hand corner.')\n",
    "\n",
    "# converting to df and assigning new names to the columns\n",
    "df_value_counts = pd.DataFrame(value_counts)\n",
    "df_value_counts = df_value_counts.reset_index()\n",
    "df_value_counts.columns = ['dates', 'number_of_articles']\n",
    "df_value_counts\n",
    "\n",
    "alt.Chart(df_value_counts, title =('Article numbers by date for')+\" \" + (street) + ' ' + 'from' +\" \"+ str(start_date)+\" \" + 'to' + \" \" + str(end_date) ).mark_bar().encode(\n",
    "   \n",
    "    alt.X('dates'),\n",
    "    alt.Y('number_of_articles'),\n",
    "    tooltip = [alt.Tooltip('dates'),\n",
    "               alt.Tooltip('number_of_articles')\n",
    "               \n",
    "              ]\n",
    "    \n",
    ").interactive()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ff188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creates graph of article numbers by street address \n",
    "\n",
    "\n",
    "address_counts = merged_df['search_term'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "print(('Article numbers by address for')+\" \" + (street) + ' ' + 'from' +\" \"+ str(start_date)+\" \" + 'to' + \" \" + str(end_date)) \n",
    "\n",
    "print( \" \")\n",
    "\n",
    "print ('Hover over the data point with mouse to get precise details, scroll to focus on specific graph areas.')\n",
    "       \n",
    "print ('The graph can be saved using the three dot tool in the upper right hand corner.')\n",
    "\n",
    "# converting to df and assigning new names to the columns\n",
    "df_address_counts = pd.DataFrame(address_counts)\n",
    "df_address_counts = df_address_counts.reset_index()\n",
    "df_address_counts.columns = ['address', 'number_of_articles']\n",
    "#extracting integers from address, sorting via streetnumber\n",
    "df_address_counts['street_number'] = df_address_counts['address'].str.replace (r'\\D+', '', regex=True)\n",
    "df_address_counts['street_number'] =   df_address_counts['street_number'].astype(int)                                                                       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_address_counts.sort_values(by='street_number', inplace =True)\n",
    "\n",
    "\n",
    "\n",
    "alt.Chart(df_address_counts, title =('Article numbers by address for')+\" \" + (street) + ' ' + 'from' +\" \"+ str(start_date)+\" \" + 'to' + \" \" + str(end_date) ).mark_bar().encode(\n",
    "   \n",
    "    alt.X('street_number'),\n",
    "    alt.Y('number_of_articles'),\n",
    "    tooltip = [alt.Tooltip('address'),\n",
    "               alt.Tooltip('number_of_articles')\n",
    "               \n",
    "              ]\n",
    "    \n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cddb09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying consolidated street level dataframe\n",
    "\n",
    "print (\"All the articles found for\" +\" \" + (street) + ' ' + 'from' +\" \"+ str(start_date)+\" \" + 'to' + \" \" + str(end_date)+\".\") \n",
    " \n",
    "print (\" \")\n",
    "\n",
    "print (\"The complete dataset, including full article  text, URLS and people list, can be found as a saved CSV file in the \" + \" \" + street_name_dates + \" \" + \"folder in your active path.\") \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_colwidth', 300) \n",
    "\n",
    "         \n",
    "\n",
    "display (merged_df[['search_term' , 'article_heading' ,'newspaper' ,'date' ,'page','people_in_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c51a08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating wordclouds based on headings and article text\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "text= merged_df[\"article_text\"]\n",
    "headings_text = merged_df[\"article_heading\"]\n",
    "text = str(text)\n",
    "headings_text = str(headings_text)\n",
    "\n",
    "\n",
    "\n",
    "wc = WordCloud(width=600, height=300, collocations=True,regexp=r\"\\w+\", min_word_length= 3 )\n",
    "hwc = WordCloud(width=600, height=300, collocations=True,regexp=r\"\\w+\", min_word_length= 3 )\n",
    "processed_text = wc.process_text (text)\n",
    "processed_heading = hwc.process_text (headings_text)\n",
    "processed_text = str(processed_text)\n",
    "processed_headindig = str(processed_heading)\n",
    "wc.generate(processed_text)\n",
    "processed_heading = str(processed_heading)\n",
    "hwc.generate(processed_heading)\n",
    "\n",
    "# Display and save the wordcloud\n",
    "print(\"Wordcloud based on heading text.\")\n",
    "display (hwc.to_image())\n",
    "hwc.to_file(Path(street_path, f'{street_name_dates} headings text .png'))\n",
    "\n",
    "print(\"Wordcloud based on article text.\")\n",
    "display (wc.to_image())\n",
    "wc.to_file(Path(street_path, f'{street_name_dates} article text .png'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4aa11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create and display the most common Trigrams (groups of words in text)\n",
    "\n",
    "text = text.encode('utf-8').decode ('ascii','ignore')\n",
    "blob = TextBlob(text)\n",
    "c1 = Counter ([' '.join(l) for l in blob.ngrams(3)]).most_common(20)\n",
    "c2= str(c1)\n",
    "Path(street_path, f'{street_name_dates}top twenty trigrams.txt').write_text(c2)\n",
    "print (\"The 20 most common three word ngrams from the article text: \")\n",
    "display (c1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\" All Ngrams and wordclouds saved to\" + \" \" + street_name_dates + \" \" + \"folder in active path.\")\n",
    "\n",
    "print (\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5c079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196151e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
